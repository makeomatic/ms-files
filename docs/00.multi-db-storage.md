# Multi Database Storage

As of Nov 13 2019 service is architected in way that Redis is used as primary and only
storage/database solution

Assumption when creating the service was that data access patterns essentially comprises of mostly
`get data by id/alias/etc` and it works extremely well from responsiveness standpoint.

However, `files.list` request which allows to request filtered list of media metadata stored in
Redis is extremely slow as it uses lua, which blocks the database server to scan random data
mostly without use of indexes. Its not a problem on a high CPU machine, but in cloud environments
problems start happening in moderate datasets (~10k items)

To solve this several solutions have been outlined:

* using Redis modules - requires writing C code, complex, but wont lock the server. Application level code can generally remain the same
* move filtering to the client-side to avoid server lock. Generally slow as we'll lock the application layer instead and is prone to race conditions (new data added during filtering)
* create & maintain indices in redis so that we don't need to run scan. could be cumbersome to maintain
* use a second database, which provides filtering capabilities and easy index management - requires to change a lot of application level code, sync existing dataset into a new database of choice and maintain parity

## Planned development

As an ideal solution Redis module seems like the best choice, but would take a lot of time to implement.
Manual index management is still too complex, so the plan so far is to do the following:

1. Introduce new database - couchdb - schemaless, easy to run/start (auto)cluster for HA/replication/sharding
2. Abstract away working with databases into high-level API for generic operations
3. Write redis & couchdb low-level implementations for high-level API
4. Write redis -> couch db syncer
5. Conditionally enable reading/writing to different databases based on enabled plugins

## Data Flow

### Uploading Media

Using file service implies a particular data flow, as of this is how the service is supposed to work:

1. Request to initiate file upload -> `files.upload`
2. Upload files to appropriate cloud storage (currently only google cloud storage is supported)
3. GCS/other storage sends webhooks or pubsub notifications about file uploads to `files.finish`
4. Once all file parts have been uploaded - `files.finish` triggers `files.process`
5. post-processing performs appropriate validations and marks upload as failed/completed

At this stage upload can be considered completed.

### Reading Media

1. `files.info` - returns information about a particular upload by id, doesnt mutate data
2. `files.download` - returns links for accessing files on GCS, links could be signed or direct
3. `files.list` - returns filtered list of files for a particular user/complete system
4. `files.head` - tells whether upload exists or not
5. `files.report` - returns amount of storage used by files

### Updating Media metadata

1. `files.update` - mutates alias, tags, etc on an existing upload
2. `files.tag.add` - adds new tags if they dont exist
3. `files.remove` - removes the upload completely
4. `files.sync` - when file isnt uploaded or notifications arent being received we need to cleanup
5. `files.access` - mutate access level to a pack of files

## Completion checklist of code migration

- [ ] files.upload
- [ ] files.finish
- [ ] files.process

- [ ] files.update
- [ ] files.tag.add
- [ ] files.tag.access
- [ ] files.tag.remove
- [ ] files.tag.sync

- [ ] files.info
- [ ] files.download
- [ ] files.list
- [ ] files.head
- [ ] files.report
